{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainp = \"original/annotated_fb_data_train.txt\"\n",
    "testp = \"original/annotated_fb_data_test.txt\"\n",
    "validp = \"original/annotated_fb_data_valid.txt\"\n",
    "labelp = \"aux/labels.map\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getdatafor(datap):\n",
    "    questions = []\n",
    "    entc = {}\n",
    "    relc = {}\n",
    "    subjc = {}\n",
    "    maxsentlen = 0\n",
    "    maxwordlen = 0\n",
    "    maxsentcharlen = 0\n",
    "    c = 0\n",
    "    for line in open(datap):\n",
    "        s, p, o, q = (line[:-1] if line[-1] == \"\\n\" else line).split(\"\\t\")\n",
    "        maxsentcharlen = max(maxsentcharlen, len(q))\n",
    "        ws = q.split()\n",
    "        questions.append((q, s, p, o))\n",
    "        for (e, col) in zip([s, o, p, s], [entc, entc, relc, subjc]):\n",
    "            if e not in col:\n",
    "                col[e] = 0\n",
    "            col[e] += 1\n",
    "        maxsentlen = max(maxsentlen, len(ws))\n",
    "        for w in ws:\n",
    "            maxwordlen = max(maxwordlen, len(w))\n",
    "            if w not in wordc:\n",
    "                wordc[w] = 0\n",
    "            wordc[w] += 1\n",
    "        if c % 1e4 == 0:\n",
    "            print c\n",
    "        c += 1\n",
    "    return questions, subjc, entc, relc, wordc, maxsentlen, maxwordlen, maxsentcharlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "0\n",
      "10000\n",
      "20000\n",
      "0\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "_, tsubjc, tentc, trelc, twc, tmsl, tmwl, tmscl = getdatafor(trainp)\n",
    "_, xsubjc, xentc, xrelc, xwc, xmsl, xmwl, xmscl = getdatafor(testp)\n",
    "_, vsubjc, ventc, vrelc, vwc, vmsl, vmwl, vmscl = getdatafor(validp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISTINCT WORDS:\t77167\n",
      "MAX SENT LENS\n",
      "train:\t33\n",
      "valid:\t20\n",
      "test:\t24\n",
      "MAX SENT CHAR LENS\n",
      "train:\t196\n",
      "valid:\t113\n",
      "test:\t141\n",
      "MAX WORD LENS\n",
      "train:\t67\n",
      "valid:\t49\n",
      "test:\t57\n"
     ]
    }
   ],
   "source": [
    "# max lens\n",
    "allwords = set(vwc.keys()).union(set(xwc.keys())).union(set(twc.keys()))\n",
    "print \"DISTINCT WORDS:\\t%d\" % len(allwords)\n",
    "print \"MAX SENT LENS\"\n",
    "print \"train:\\t%d\" % tmsl\n",
    "print \"valid:\\t%d\" % vmsl\n",
    "print \"test:\\t%d\" % xmsl\n",
    "print \"MAX SENT CHAR LENS\"\n",
    "print \"train:\\t%d\" % tmscl\n",
    "print \"valid:\\t%d\" % vmscl\n",
    "print \"test:\\t%d\" % xmscl\n",
    "print \"MAX WORD LENS\"\n",
    "print \"train:\\t%d\" % tmwl\n",
    "print \"valid:\\t%d\" % vmwl\n",
    "print \"test:\\t%d\" % xmwl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL NUMBER OF DISTINCT ENTITIES\n",
      "total: 131684\n",
      "train: 95832\n",
      "valid: 16000\n",
      "test:  30476\n",
      "TOTAL NUMBER OF DISTINCT RELATIONS\n",
      "total: 1837\n",
      "train: 1629\n",
      "valid: 783\n",
      "test:  1034\n"
     ]
    }
   ],
   "source": [
    "# total number of distinct entities:\n",
    "allents = set(tentc.keys()).union(set(xentc.keys())).union(set(ventc.keys()))\n",
    "print \"TOTAL NUMBER OF DISTINCT ENTITIES\"\n",
    "print \"total: %d\" % len(allents)\n",
    "print \"train: %d\" % len(tentc)\n",
    "print \"valid: %d\" % len(ventc)\n",
    "print \"test:  %d\" % len(xentc)\n",
    "allrels = set(trelc.keys()).union(set(xrelc.keys())).union(set(vrelc.keys()))\n",
    "print \"TOTAL NUMBER OF DISTINCT RELATIONS\"\n",
    "print \"total: %d\" % len(allrels)\n",
    "print \"train: %d\" % len(trelc)\n",
    "print \"valid: %d\" % len(vrelc)\n",
    "print \"test:  %d\" % len(xrelc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF ENTITIES IN TEST BUT NOT IN TRAIN\n",
      "24121\n",
      "NUMBER OF RELATIONS IN TEST BUT NOT IN TRAIN\n",
      "148\n"
     ]
    }
   ],
   "source": [
    "# number of entities in test but not in train\n",
    "xonlyents = set(xentc.keys()).difference(set(tentc.keys()))\n",
    "print \"NUMBER OF ENTITIES IN TEST BUT NOT IN TRAIN\"\n",
    "print len(xonlyents)\n",
    "xonlyrels = set(xrelc.keys()).difference(set(trelc.keys()))\n",
    "print \"NUMBER OF RELATIONS IN TEST BUT NOT IN TRAIN\"\n",
    "print len(xonlyrels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"allents.col\", \"w\") as f:\n",
    "    for ent in allents.union(allrels):\n",
    "        f.write(\"%s\\n\" % ent)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load labels:\n",
    "labels = {}\n",
    "for line in open(labelp):\n",
    "    x, y = line[:-1].split(\"\\t\")\n",
    "    labels[x] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130822"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sre = re.compile(\"www\\.freebase\\.com/m/(.+)\")\n",
    "pre = re.compile(\"www\\.freebase\\.com(.+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m.04whkz5 E\n",
      "m.0tp2p24 Cardiac Arrest\n",
      "m.04j0t75 The Debt\n",
      "m.0ftqr Nobuo Uematsu\n",
      "m.036p007 Eve-Olution\n",
      "m.0ms5mg Most of Us Are Sad\n",
      "m.086k8 Warner Bros. Entertainment\n",
      "m.02vnx8y Don Graham\n",
      "m.01smm Columbus\n",
      "m.0mgb6cl Tibet\n",
      "m.02dtg Detroit\n",
      "m.0275d7v RYNA\n"
     ]
    }
   ],
   "source": [
    "c = 10\n",
    "for line in open(trainp):\n",
    "    s, _, _, _ = line.split(\"\\t\")\n",
    "    s = \"m.\"+sre.match(s).group(1)\n",
    "    print s, labels[s]\n",
    "    if c < 0:\n",
    "        break\n",
    "    c -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traino = \"fb_train.tsv\"\n",
    "testo = \"fb_test.tsv\"\n",
    "valido = \"fb_valid.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "0\n",
      "10000\n",
      "0\n",
      "10000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "for (x, y) in zip([trainp, validp, testp], [traino, valido, testo]):\n",
    "    questions, _, _, _, _, _, _ = getdatafor(x)\n",
    "    with open(y, \"w\") as f:\n",
    "        for q in questions:\n",
    "            s = q[1]\n",
    "            p = q[2]\n",
    "            s = \"m.\" + sre.match(s).group(1)\n",
    "            p = pre.match(p).group(1)\n",
    "            f.write(\"%s\\t%s %s\\n\" % (q[0], s, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
